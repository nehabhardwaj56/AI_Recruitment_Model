# -*- coding: utf-8 -*-
"""resume_screenng_&_prediction (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1abC3oh6wRX_tc-qGZuZuGWE34IxWTHkE
"""

import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from transformers import pipeline
nltk.download('stopwords')

print(combined_df.columns)

"""# Resume screening"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans

# Load your dataset
# Replace 'your_dataset.csv' with your actual dataset file
combined_df = pd.read_csv("combined_cleaned_dataset.csv")

# Step 1: Role-specific thresholds for screening
role_thresholds = {
    "Software Engineer": 0.4,
    "Data Scientist": 0.2,
    "UI Engineer": None,  # No threshold
}

# Step 2: Function to calculate TF-IDF and cosine similarity
def calculate_similarity(df, job_col, resume_col):
    vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
    # Fit the TF-IDF vectorizer on both job descriptions and resumes
    vectorizer.fit(pd.concat([df[job_col], df[resume_col]]))

    job_desc_vector = vectorizer.transform(df[job_col])
    resume_vector = vectorizer.transform(df[resume_col])

    # Compute cosine similarity
    df['similarity'] = cosine_similarity(job_desc_vector, resume_vector).diagonal()
    return df

# Step 3: Apply role-specific thresholds for screening
def screen_resumes(df, role_thresholds):
    decisions = []
    for _, row in df.iterrows():
        role = row['Role']
        similarity = row['similarity']
        threshold = role_thresholds.get(role, None)

        # Apply decision logic
        if threshold is None or similarity >= threshold:
            decisions.append("Selected")
        else:
            decisions.append("Rejected")

    df['decision'] = decisions
    return df

# Step 4: Add clustering for job descriptions (optional)
def cluster_job_descriptions(df, job_col, n_clusters=5):
    vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
    X = vectorizer.fit_transform(df[job_col])
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    df['job_desc_cluster'] = kmeans.fit_predict(X)
    return df

# Step 5: Execute the workflow
combined_df = calculate_similarity(combined_df, 'Job Description', 'Resume')
combined_df = screen_resumes(combined_df, role_thresholds)
combined_df = cluster_job_descriptions(combined_df, 'Job Description')

# Handle missing IDs

combined_df['ID'] = combined_df['ID'].fillna('').astype(str)
missing_ids = combined_df['ID'] == ''
combined_df.loc[missing_ids, 'ID'] = [f'auto_{i}' for i in range(1, missing_ids.sum() + 1)]


# Save and display results
combined_df.to_csv('resume_screening_results.csv', index=False)
print(combined_df[['ID', 'Role', 'similarity', 'decision', 'job_desc_cluster']])


# Save the results to a new CSV file
#combined_df.to_csv('resume_screening_results.csv', index=False)

# Display the output
#print(combined_df[['ID', 'Role', 'similarity', 'decision', 'job_desc_cluster']])

print(combined_df['decision'].value_counts())

print(combined_df.groupby('Role')['decision'].value_counts())

print(combined_df.groupby('Role')['similarity'].mean())

# Step 1: Analyze Similarity Scores Across Roles
role_similarity_stats = combined_df.groupby('Role')['similarity'].describe()
print(role_similarity_stats)

# Step 2: Define Role-Specific Thresholds Based on Analysis
role_thresholds = {
    'Software Engineer': 0.4,
    'Data Scientist': 0.2,
    'UI Engineer': None,  # No threshold
    'Data Engineer': 0.3,
    'Product Manager': 0.25,
}

# Step 3: Apply the Thresholds
def apply_role_thresholds(row, thresholds):
    role = row['Role']
    similarity = row['similarity']
    threshold = thresholds.get(role, None)

    # If no threshold, default to 'Selected'
    if threshold is None:
        return 'Selected'
    return 'Selected' if similarity >= threshold else 'Rejected'

combined_df['decision'] = combined_df.apply(
    apply_role_thresholds, axis=1, thresholds=role_thresholds
)

# Display the updated DataFrame
print(combined_df[['ID', 'Role', 'similarity', 'decision']])

from sklearn.preprocessing import LabelEncoder

#Separate features and target
X = combined_df.drop(['decision', 'ID'], axis=1, errors='ignore')
y = combined_df['decision']

# Identify categorical and numeric columns
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
numeric_cols = X.select_dtypes(include=['number']).columns.tolist()

# Define preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ]
)

# Transform the data
X_preprocessed = preprocessor.fit_transform(X)

# Encode the target variable
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)  # Encode target labels

from sklearn.model_selection import train_test_split

# Perform train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_preprocessed, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)
print(f"Train set size: {X_train.shape}, Test set size: {X_test.shape}")

# Manually impute missing values in numeric and categorical columns
X[numeric_cols] = X[numeric_cols].apply(lambda col: col.fillna(col.mean()))
X[categorical_cols] = X[categorical_cols].apply(lambda col: col.fillna(col.mode()[0]))

"""# SVM model"""

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

# Re-run the train-test split and model training
X_train, X_test, y_train, y_test = train_test_split(
    X_preprocessed, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42)
svm_model.fit(X_train, y_train)

# Make predictions
y_pred_svm = svm_model.predict(X_test)

# Evaluate the model
print("SVM Classification Report:")
print(classification_report(y_test, y_pred_svm))

print("\nSVM Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_svm))

print("\nSVM Accuracy Score:")
print(accuracy_score(y_test, y_pred_svm))

"""# Predictions"""

import pickle

# Save the preprocessor
with open('preprocessor.pkl', 'wb') as f:
    pickle.dump(preprocessor, f)

# Save the SVM model
with open('svm_model.pkl', 'wb') as f:
    pickle.dump(svm_model, f)

import pandas as pd

# Load the Excel file
new_data = pd.read_excel('Copy of prediction_data.xlsx')  # Adjust the filename if needed

# Display the columns of the new data
print(new_data.columns)

from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
import pandas as pd

# Assuming you have your training data available
# Example: Replace this with your actual training data
X_train = pd.DataFrame({
    'Transcript': ['Interview went well', 'Candidate was not qualified', 'Strong skills in the area', 'Not a good fit for the role'],
    'Job Description': ['Software Engineer role', 'Data Scientist role', 'Developer role', 'HR role']
})

y_train = [1, 0, 1, 0]  # Example labels for training (1: selected, 0: rejected)

# Define your preprocessor (to use TF-IDF for text columns)
preprocessor = ColumnTransformer(
    transformers=[
        ('text', TfidfVectorizer(), 'Transcript'),  # Transform 'Transcript' column using TF-IDF
        ('text2', TfidfVectorizer(), 'Job Description')  # Transform 'Job Description' column using TF-IDF
    ])

# Step 1: Fit the preprocessor using training data
# Fit the preprocessor with the training dataset (X_train)
preprocessor.fit(X_train)

# Step 2: Create and train your SVM model in a pipeline (this combines the preprocessor and the model)
model_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('svm', SVC(kernel='linear'))  # You can change the SVM model as needed
])

# Train the model with the training data
model_pipeline.fit(X_train, y_train)

# Step 3: Load the new data (prediction data)
# Example: Replace this with your actual new dataset for prediction
# Correct the file path by putting it inside quotes
import os
# Ensure the current directory is correct (this should be the directory where your files are located)
current_dir = os.getcwd()

# Set the path for the prediction file (replace with the actual name of your file)
current_dir = os.getcwd()
prediction_file_path = os.path.join(current_dir, 'Copy of prediction_data (1).xlsx')
new_data = pd.read_excel(prediction_file_path)

 # Ensure this path is correct

# Step 4: Process new data (only transformation, not fitting again)
# We don't need to fit the preprocessor again; we just transform the new data.
X_new = new_data[['Transcript', 'Job Description']]  # Ensure new data contains the correct columns

# Step 5: Make predictions using the trained model pipeline (which includes the fitted preprocessor)
y_new_pred = model_pipeline.predict(X_new)

# Step 6: Display the predictions
print("Predictions:", y_new_pred)

"""# '1'is for selection
# '0' is for rejection
"""

# Assuming you have already fitted your model (e.g., model_pipeline)
# and that your preprocessor is included in the pipeline
X_new = new_data[['Transcript', 'Job Description']]  # Make sure this contains the right columns
predictions = model_pipeline.predict(X_new)  # Get predictions from the model

# Now you can add predictions to your new_data
new_data['Predictions'] = predictions

# Visualize the results
print(new_data[['Transcript', 'Job Description', 'Predictions']].head())

from sklearn.metrics import accuracy_score, classification_report

# Assuming 'Actual Outcome' is the true label column in your new_data
y_true = new_data['Predictions']

# Calculate accuracy and other metrics
accuracy = accuracy_score(y_true, predictions)
print(f'Accuracy: {accuracy}')

# Print detailed classification report
print(classification_report(y_true, predictions))

# Save the predictions to a new Excel file
new_data.to_excel('predictions_output.xlsx', index=False)

"""# Insights"""

# Filter candidates predicted to be selected (assuming 1 means selected and 0 means rejected)
selected_candidates = new_data[new_data['Predictions'] == 1]
rejected_candidates = new_data[new_data['Predictions'] == 0]

print('Selected Candidates:')
print(selected_candidates)

"""#  Automatic E-mail generation"""

import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

# Example function to send an email
def send_email(receiver_email, subject, body):
    sender_email = "your_email@example.com"  # Your email address
    sender_password = "your_password"  # Your email password (consider using OAuth for security)

    # Create the email headers and body
    message = MIMEMultipart()
    message['From'] = sender_email
    message['To'] = receiver_email
    message['Subject'] = subject

    # Attach the body with the message
    message.attach(MIMEText(body, 'plain'))

    try:
        # Connect to the mail server and send the email
        with smtplib.SMTP('smtp.example.com', 587) as server:
            server.starttls()
            server.login(sender_email, sender_password)
            server.sendmail(sender_email, receiver_email, message.as_string())
        print(f"Email sent to {receiver_email}")
    except Exception as e:
        print(f"Failed to send email to {receiver_email}: {e}")

# 3. Filter the selected candidates from the predictions
selected_candidates = new_data[new_data['Predictions'] == 'Selected']

# 4. Generate automatic emails for the selected candidates
# You would replace 'candidate_emails' with a list of emails (if available)
# For this case, I'll use placeholder email addresses:

candidate_emails = {
    'Candidate 1': 'candidate1@example.com',
    'Candidate 2': 'candidate2@example.com',
    'Candidate 3': 'candidate3@example.com',
    # Add more candidates and emails here
}

subject = "Congratulations on Your Job Interview"
body_template = """
Dear {name},

We are pleased to inform you that you have been selected for the interview process.

Best regards,
Your Company
"""

# Iterate through the selected candidates
for _, candidate in selected_candidates.iterrows():
    candidate_name = candidate['Name']
    if candidate_name in candidate_emails:
        receiver_email = candidate_emails[candidate_name]
        body = body_template.format(name=candidate_name)
        send_email(receiver_email, subject, body)
#Assuming Neha has been selected, and you are sending the email:
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

# SMTP server configuration
smtp_server = "smtp.gmail.com"
smtp_port = 587
sender_email = "your_email@gmail.com"  # Your email address
sender_password = "your_email_password"  # Your email password

# Recipient email address (Neha's email)
receiver_email = "nehabhardwaj3603@gmail.com"

# Create the email content
subject = "Congratulations on Your Job Interview"
body = """
Dear Neha,

We are pleased to inform you that you have been selected for the interview process at Our Company.
We will reach out to you shortly with further details.

Best regards,
Your Company
"""

# Create the email
message = MIMEMultipart()
message["From"] = sender_email
message["To"] = receiver_email
message["Subject"] = subject

# Attach the body to the email
message.attach(MIMEText(body, "plain"))

# Send the email
try:
    # Set up the SMTP server and send the email
    server = smtplib.SMTP(smtp_server, smtp_port)
    server.starttls()  # Secure the connection
    server.login(sender_email, sender_password)
    server.sendmail(sender_email, receiver_email, message.as_string())
    print(f"Email sent to {receiver_email}")
    server.quit()  # Close the connection
except Exception as e:
    print(f"Failed to send email to {receiver_email}: {str(e)}")





