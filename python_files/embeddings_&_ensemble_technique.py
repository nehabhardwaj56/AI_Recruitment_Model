# -*- coding: utf-8 -*-
"""Embeddings_&_Ensemble_Technique.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14hqx9CEll0UPcW7bhU8rMWYv9sdh9FOz

#Import Libraries and Load Dataset
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from transformers import BertTokenizer, BertModel
import torch
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
import warnings
warnings.filterwarnings("ignore")

# Load the cleaned dataset
file_path = "combined_cleaned_dataset.csv"  # Update with your file name
combined_df = pd.read_csv(file_path)

combined_df.head()

combined_df.info()

"""#Generate BERT Embeddings"""

# Initialize BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased')

"""#Purpose:
 Initializes the BERT tokenizer and model for generating embeddings.
#Explanation:
BertTokenizer is used to tokenize the text data.

BertModel is the pre-trained BERT model that will be used to generate embeddings.
"""

def get_bert_embeddings(text_column):
    """
    Generate BERT embeddings for a given text column.
    """
    embeddings = []
    for text in text_column:
        inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
        outputs = bert_model(**inputs)
        cls_embedding = outputs.last_hidden_state[:, 0, :].detach().numpy()
        embeddings.append(cls_embedding.flatten())
    return np.array(embeddings)

"""#Bert Embeddings for Transcript"""

# Generate embeddings
combined_df['Transcript_embeddings'] = list(get_bert_embeddings(combined_df['Transcript']))

"""#Bert Embeddings for Resume"""

combined_df['Resume_embeddings'] = list(get_bert_embeddings(combined_df['Resume']))

"""#Bert Embeddings for Job_Description"""

combined_df['Job_Description_embeddings'] = list(get_bert_embeddings(combined_df['Job Description']))

combined_df.head()

combined_df.info()

# Assuming 'data' is your pandas DataFrame
unique_decisions = combined_df['decision'].unique()
print("Unique values in 'decision' column:", unique_decisions)

# Assuming 'data' is your pandas DataFrame
combined_df['decision'] = combined_df['decision'].replace({'select': 'selected', 'reject': 'rejected'})

# Assuming 'data' is your pandas DataFrame
unique_decisions = combined_df['decision'].unique()
print("Unique values in 'decision' column:", unique_decisions)

"""#Combine Features and Train-Test Split"""

# Combine embeddings and features
handcrafted_features = ['Transcript_length', 'Resume_length', 'Job_Description_length']
X_embeddings = np.hstack([
    np.vstack(df['Transcript_embeddings']),
    np.vstack(df['Resume_embeddings']),
    np.vstack(df['Job_Description_embeddings']),
    combined_df[handcrafted_features].values
])

# Encode target variable
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(df['decision'])

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_embeddings, y, test_size=0.2, random_state=42, stratify=y)

print(np.unique(y_test))

"""#Random Forest Model"""

# Train Random Forest
rf = RandomForestClassifier(random_state=42, n_estimators=100)
rf.fit(X_train, y_train)

# Evaluate Random Forest
y_pred_rf = rf.predict(X_test)
y_prob_rf = rf.predict_proba(X_test)[:, 1]

print("Random Forest - Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Random Forest - ROC AUC Score:", roc_auc_score(y_test, y_prob_rf))

"""#Artificial Neural Network (ANN)"""

import tensorflow as tf
from tensorflow.keras.optimizers import Adam

# Define ANN model
ann = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

# Compile the ANN
ann.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Train the ANN
history = ann.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)

# Evaluate the ANN
y_prob_ann = ann.predict(X_test).flatten()
y_pred_ann = (y_prob_ann > 0.5).astype(int)

print("ANN - Accuracy:", accuracy_score(y_test, y_pred_ann))
print("ANN - ROC AUC Score:", roc_auc_score(y_test, y_prob_ann))

"""#Combine Random Forest and ANN Predictions"""

# Combine predictions from ANN and Random Forest
combined_predictions = (y_prob_rf + y_prob_ann) / 2
combined_pred_labels = (combined_predictions > 0.5).astype(int)

# Evaluate combined model
print("Combined Model - Accuracy:", accuracy_score(y_test, combined_pred_labels))
print("Combined Model - ROC AUC Score:", roc_auc_score(y_test, combined_predictions))

# Save the combined dataset with embeddings for future use
df.to_csv("combined_dataset_with_embeddings.csv", index=False)
print("Dataset with embeddings saved as 'combined_dataset_with_embeddings.xlsx'")









