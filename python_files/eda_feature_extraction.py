# -*- coding: utf-8 -*-
"""Eda_feature_extraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17VNAp_g6sZ1xjrBlOPdKvxsuabCOhUdM
"""

import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from transformers import pipeline
nltk.download('stopwords')

# Specify the folder path where your datasets are located
folder_path = 'ai_assign'  # Update this to the folder where the datasets are located

# Initialize an empty list to store cleaned DataFrames
cleaned_dataframes = []

# List all Excel files in the folder
files = [f for f in os.listdir(folder_path) if f.endswith('.xlsx')]

# Loop through each file in the folder
for file in files:
    file_path = os.path.join(folder_path, file)
    print(f"Processing file: {file}")

    # Load the dataset
    df = pd.read_excel(file_path)

    # Print column names for verification
    print(f"Columns in {file}: {df.columns.tolist()}")

    # Drop duplicates
    df.drop_duplicates(inplace=True)

    # Define critical columns
    required_columns = ['Transcript', 'Resume', 'Job Description', 'Decision']
    available_columns = [col for col in required_columns if col in df.columns]

    if available_columns:
        # Drop rows with missing values in critical columns
        df.dropna(subset=available_columns, inplace=True)
    else:
        print(f"Warning: None of the critical columns {required_columns} are found in {file}.")

    # Fill missing values in non-critical columns
    df.fillna('Not Specified', inplace=True)
      # Append source file name for tracking
    df['Source_File'] = file

    # Append cleaned DataFrame to the list
    cleaned_dataframes.append(df)

# Combine all cleaned DataFrames into one
combined_df = pd.concat(cleaned_dataframes, ignore_index=True)
print(f"Combined dataset shape: {combined_df.shape}")

# Function to clean text columns
def clean_text_column(column):
    """
    Cleans a text column by:
    1. Removing all non-alphabetic characters.
    2. Removing extra spaces.
    3. Converting text to lowercase.
    """
    if column.dtype == 'object':  # Only apply to text columns
        return column.str.replace(r"[^a-zA-Z\s]", "", regex=True).str.strip().str.lower()
    return column

# Clean specific text columns
columns_to_clean = ['Transcript', 'Resume', 'Job Description', 'Reason for decision']
for col in columns_to_clean:
    if col in combined_df.columns:
        combined_df[col] = clean_text_column(combined_df[col])

# Display cleaned data for verification
print("Cleaned Data Sample:")
print(combined_df.head())

# Generate summary insights
insights = {}

# Total Candidates
insights['Total Candidates'] = len(combined_df)

# Selected and Rejected Candidates
if 'Decision' in combined_df.columns:
    decision_counts = combined_df['Decision'].str.strip().value_counts()
    insights['Selected Candidates'] = decision_counts.get('select', 0)
    insights['Rejected Candidates'] = decision_counts.get('reject', 0)
else:
    insights['Selected Candidates'] = "Column 'Decision' not found"
    insights['Rejected Candidates'] = "Column 'Decision' not found"

# Most Common Reason for Decision
if 'Reason for decision' in combined_df.columns:
    insights['Most Common Reason for Decision'] = combined_df['Reason for decision'].mode()[0]
else:
    insights['Most Common Reason for Decision'] = "Column 'Reason for decision' not found"

# Print insights
print("Insights:")
for key, value in insights.items():
    print(f"{key}: {value}")

# Check for null values
print("Null values in combined dataset:")
print(combined_df.isnull().sum())

# Basic statistics for numeric columns
print("Basic statistics for numeric columns:")
print(combined_df.describe())

print(combined_df.columns.tolist())

excel_files

['dataset_1_2_3_combined','dataset4.xlsx','dataset5.xlsx','dataset6.xlsx','dataset7.xlsx','dataset8.xlsx','dataset9,xlsx']

dataframes= {}
for file in excel_files:
    file_path=os.path.join(folder_path,file)
    dataframes[file]=pd.read_excel(file_path)
    print(f"Loaded{file} with shape{dataframes[file].shape}")
    combined_df=pd.concat(dataframes.values(),ignore_index=True)

import os
import pandas as pd

# Folder containing the Excel files
folder_path = "ai_assign"

# Collect Excel files from the folder
excel_files = [f for f in os.listdir(folder_path) if f.endswith(".xlsx")]

# Loop through each file and print column names
for file in excel_files:
    file_path = os.path.join(folder_path, file)

    # Load the Excel file into a DataFrame
    df = pd.read_excel(file_path)

    # Print the file name and its columns
    print(f"Columns in {file}:")
    print(df.columns.tolist())  # Convert to a list and print
    print("-" * 50)  # Separator between file outputs

combined_df.drop(columns=["Unnamed: 0"], inplace=True, errors="ignore")

combined_df.rename(columns={"decision": "performance_(select/reject)"}, inplace=True)

for file in excel_files:
    file_path = os.path.join(folder_path, file)
    df = pd.read_excel(file_path)

    # Rename columns for consistency
    df.rename(columns={
        'performance': 'performance_(select/reject)',
        'job description': 'job_description',
        'reason for decision': 'reason_for_decision',
        'name': 'name',
        'role': 'role',
        'resume': 'resume',
        'transcript': 'transcript'
    }, inplace=True)

    # Re-append cleaned data
    dataframes[file] = df

combined_df.columns

combined_df['Role'].value_counts()

combined_df=pd.concat(dataframes.values(),ignore_index=True)
print(combined_df.info())
print(combined_df.describe())

combined_df.isnull().sum()

print(combined_df.columns)

combined_df['decision'].value_counts()

combined_df['length_of_transcript']=combined_df['Transcript'].apply(lambda x:len(x))

combined_df[['length_of_transcript','decision']]

print(combined_df.columns)

# Group by Job_Description and count resumes
job_description_counts = combined_df.groupby('Job Description').size().reset_index(name='Resume_Count')

print(job_description_counts)

print(combined_df.isnull().sum())

print(combined_df.describe())

combined_df.hist(figsize=(14, 10), bins=20)
plt.show()





from transformers import BertTokenizer, BertModel

# Load pre-trained BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Sample text data
texts = combined_df['Transcript'].tolist()

# Tokenize the text data
tokens = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')

# Get BERT embeddings
with torch.no_grad():
    outputs = model(**tokens)
    embeddings = outputs.last_hidden_state

# Convert embeddings to a numpy array
embeddings = embeddings.numpy()

!pip install wordcloud

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 8))
sns.countplot(data=combined_df, x='Role', order=combined_df['Role'].value_counts().index, palette='viridis')
plt.title('Distribution of Job Roles')
plt.xlabel('Role')
plt.ylabel('Count')
plt.xticks(rotation=90)
plt.show()

plt.figure(figsize=(12, 8))
sns.countplot(data=combined_df, x='decision', hue='Role', palette='pastel')
plt.title('Count of decisions by Role')
plt.xlabel('decision')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.legend(title='Role')
plt.show()

combined_df['Word_Count'] = combined_df['Transcript'].apply(lambda x: len(str(x).split()))

plt.figure(figsize=(12, 8))
sns.histplot(data=combined_df, x='Word_Count', bins=30, kde=True, color='green')
plt.title('Word Count Distribution in Transcripts')
plt.xlabel('Word Count')
plt.ylabel('Frequency')
plt.show()

from sklearn.preprocessing import LabelEncoder

# Encode 'Role' and 'Decision' columns
le_role = LabelEncoder()
combined_df['Role_Encoded'] = le_role.fit_transform(combined_df['Role'])

le_decision = LabelEncoder()
combined_df['decision_Encoded'] = le_decision.fit_transform(combined_df['decision'])

print(combined_df.head())

plt.figure(figsize=(12, 8))
sns.countplot(data=combined_df, x='Role', hue='decision', palette='pastel')
plt.title('Distribution of decisions by Role')
plt.xlabel('Role')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.legend(title='decision')
plt.show()

combined_df['Transcript_Length'] = combined_df['Transcript'].apply(lambda x: len(str(x).split()))

plt.figure(figsize=(12, 8))
sns.boxplot(data=combined_df, x='Role', y='Transcript_Length', palette='Set3')
plt.title('Transcript Length Distribution by Role')
plt.xlabel('Role')
plt.ylabel('Transcript Length')
plt.xticks(rotation=45)
plt.show()

from scipy.stats import chi2_contingency

# Contingency table for Role and Decision
contingency_table = pd.crosstab(combined_df['Role'], combined_df['decision'])

# Chi-square test
chi2, p, dof, expected = chi2_contingency(contingency_table)

print(f"Chi-square test statistic: {chi2}")
print(f"P-value: {p}")

import pandas as pd
import numpy as np

# Load your dataset
combined_df = pd.concat(dataframes.values(),ignore_index=True)

# Check the first few rows
print(combined_df.head())

# Extract the 'Role' and 'Transcript' columns
roles = combined_df['Role'].values
transcripts = combined_df['Transcript'].values

# Compute the length of each transcript
transcript_lengths = np.array([len(str(transcript).split()) for transcript in transcripts])

# Create a dictionary to store the lengths of transcripts for each role
role_transcript_lengths = {}

for role, length in zip(roles, transcript_lengths):
    if role not in role_transcript_lengths:
        role_transcript_lengths[role] = []
    role_transcript_lengths[role].append(length)

# Calculate statistics for each role
role_statistics = {role: (np.mean(lengths), np.std(lengths)) for role, lengths in role_transcript_lengths.items()}

# Create a structured array to store the statistics
dtype = [('Role', 'U50'), ('Mean_Length', 'f4'), ('Std_Length', 'f4')]
role_stats_array = np.array([(role, stats[0], stats[1]) for role, stats in role_statistics.items()], dtype=dtype)

# Display the array
print(role_stats_array)

!pip install textblob
from textblob import TextBlob


# Calculate sentiment polarity for each transcript
combined_df['Sentiment_Polarity'] = combined_df['Transcript'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)

# Box plot of sentiment polarity by role
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 8))
sns.boxplot(data=combined_df, x='Role', y='Sentiment_Polarity', palette='Set2')
plt.title('Sentiment Polarity Distribution by Role')
plt.xlabel('Role')
plt.ylabel('Sentiment Polarity')
plt.xticks(rotation=90)
plt.show()

plt.figure(figsize=(12, 8))
sns.countplot(data=combined_df, x='decision', hue='Role', palette='coolwarm')
plt.title('Distribution of decision Outcomes by Role')
plt.xlabel('decision')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.legend(title='Role')
plt.show()

from sklearn.feature_extraction.text import CountVectorizer

# Function to extract n-grams
def get_top_ngrams(corpus, n=None, ngram_range=(2, 2)):
    vec = CountVectorizer(ngram_range=ngram_range).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]

# Extract top bigrams
bigrams = get_top_ngrams(combined_df['Transcript'].dropna(), n=10, ngram_range=(2, 2))

# Plot top bigrams
bigrams_df = pd.DataFrame(bigrams, columns=['Bigram', 'Frequency'])
plt.figure(figsize=(12, 8))
sns.barplot(data=bigrams_df, x='Frequency', y='Bigram', palette='viridis')
plt.title('Top 10 Bigrams in Transcripts')
plt.xlabel('Frequency')
plt.ylabel('Bigram')
plt.show()

!pip install wordcloud

import pandas as pd

# Load your dataset
combined_df = pd.concat(dataframes.values(),ignore_index=True)
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Separate selected and rejected transcripts
selected_transcripts = ' '.join(combined_df[combined_df['decision'] == 'Select']['Transcript'].dropna())
rejected_transcripts = ' '.join(combined_df[combined_df['decision'] == 'Reject']['Transcript'].dropna())

# Generate word clouds
selected_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(selected_transcripts)
rejected_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(rejected_transcripts)

# Visualize the word clouds
plt.figure(figsize=(16, 8))

# Selected transcripts
plt.subplot(1, 2, 1)
plt.imshow(selected_wordcloud, interpolation='bilinear')
plt.title('Selected Transcripts')
plt.axis('off')

# Rejected transcripts
plt.subplot(1, 2, 2)
plt.imshow(rejected_wordcloud, interpolation='bilinear')
plt.title('Rejected Transcripts')
plt.axis('off')

plt.show()
from sklearn.feature_extraction.text import CountVectorizer

# Function to get top keywords
def get_top_keywords(text, n=10):
    vec = CountVectorizer(stop_words='english').fit([text])
    bag_of_words = vec.transform([text])
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
    return words_freq[:n]

# Get top keywords for selected and rejected transcripts
top_keywords_selected = get_top_keywords(selected_transcripts, n=10)
top_keywords_rejected = get_top_keywords(rejected_transcripts, n=10)

# Convert to DataFrame for visualization
keywords_selected_df = pd.DataFrame(top_keywords_selected, columns=['Keyword', 'Frequency'])
keywords_rejected_df = pd.DataFrame(top_keywords_rejected, columns=['Keyword', 'Frequency'])

# Plot top keywords
plt.figure(figsize=(16, 8))

# Selected transcripts
plt.subplot(1, 2, 1)
sns.barplot(data=keywords_selected_df, x='Frequency', y='Keyword', palette='viridis')
plt.title('Top Keywords in Select Transcripts')

# Rejected transcripts
plt.subplot(1, 2, 2)
sns.barplot(data=keywords_rejected_df, x='Frequency', y='Keyword', palette='plasma')
plt.title('Top Keywords in Reject Transcripts')

plt.show()



from sklearn.feature_extraction.text import CountVectorizer

# Extract skills from resumes
skills = combined_df['Resume'].dropna()

# Vectorize skills
vectorizer = CountVectorizer(stop_words='english')
skills_matrix = vectorizer.fit_transform(skills)
skills_sum = skills_matrix.sum(axis=0)
skills_freq = [(word, skills_sum[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
skills_freq = sorted(skills_freq, key=lambda x: x[1], reverse=True)

# Get top 20 skills
top_skills = skills_freq[:20]

# Convert to DataFrame
skills_df = pd.DataFrame(top_skills, columns=['Skill', 'Frequency'])

# Plot top skills
plt.figure(figsize=(12, 8))
sns.barplot(data=skills_df, x='Frequency', y='Skill', palette='Blues_d')
plt.title('Top 20 Skills Listed in Resumes')
plt.xlabel('Frequency')
plt.ylabel('Skill')
plt.show()

combined_df['Resume_Length'] = combined_df['Resume'].apply(lambda x: len(str(x).split()))

plt.figure(figsize=(12, 8))
sns.boxplot(data=combined_df, x='decision', y='Resume_Length', palette='coolwarm')
plt.title('Resume Length Distribution by Decision')
plt.xlabel('decision')
plt.ylabel('Resume Length')
plt.show()

















